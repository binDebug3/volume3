{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volume 3: Gibbs Sampling and LDA\n",
    "    <Name>\n",
    "    <Class>\n",
    "    <Date>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import gammaln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Implement a Gibbs sampler for the exam scores problem (using the data in `examscores.npy`).\n",
    "Test your sampler with priors $\\nu=80$, $\\tau^{2} = 16$, $\\alpha = 3$, and $\\beta = 50$, collecting $1000$ samples.\n",
    "Plot your samples of $\\mu$ and your samples of $\\sigma^{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs(y, nu, tau2, alpha, beta, n_samples):\n",
    "    \"\"\" Gibbs sampler for the exam scores problem, assuming the\n",
    "    following likelihood and priors.\n",
    "        y_i    ~ N(mu, sigma2),\n",
    "        mu     ~ N(nu, tau2),\n",
    "        sigma2 ~ IG(alpha, beta),\n",
    "\n",
    "    Parameters:\n",
    "        y ((N,) ndarray): the exam scores.\n",
    "        nu (float): The prior mean parameter for mu.\n",
    "        tau2 (float > 0): The prior variance parameter for mu.\n",
    "        alpha (float > 0): The prior alpha parameter for sigma2.\n",
    "        beta (float < 0): The prior beta parameter for sigma2.\n",
    "        n_samples (int): the number of samples to draw.\n",
    "\n",
    "    Returns:\n",
    "        ((n_samples, 2) ndarray): The mu and sigma2 samples (as columns).\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Problem 1 Incomplete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Plot the kernel density estimators for the posterior distributions of $\\mu$ and $\\sigma^2$.\n",
    "\n",
    "Next, use your samples of $\\mu$ and $\\sigma^2$ to draw samples from the posterior predictive distribution.\n",
    "Plot the kernel density estimator of your sampled scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "Complete the method `LDACGS.initialize()`.\n",
    "\n",
    "\n",
    "\n",
    "## Problem 4\n",
    "\n",
    "Complete the method `LDACGS._sweep()`.\n",
    "\n",
    "\n",
    "\n",
    "## Problem 5\n",
    "\n",
    "Complete the method `LDACGS.sample()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDACGS:\n",
    "    \"\"\" Do LDA with Gibbs Sampling. \"\"\"\n",
    "\n",
    "    def __init__(self, n_topics, alpha=0.1, beta=0.1):\n",
    "        \"\"\" Initializes attributes n_topics, alpha, and beta. \"\"\"\n",
    "        self.n_topics = n_topics\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def buildCorpus(self, filename, stopwords_file=None):\n",
    "        \"\"\" Reads the given filename, and using any provided stopwords,\n",
    "            initializes attributes vocab and documents. \n",
    "            \n",
    "            Vocab is a list of terms found in filename.\n",
    "            \n",
    "            Documents is a list of dictionaries (a dictionary for each \n",
    "            document); for dictionary m in documents, each entry is of \n",
    "            the form n:w, where w is the index in vocab of the nth word \n",
    "            in document m.\n",
    "        \"\"\"\n",
    "        with open(filename, 'r') as infile:  # create vocab\n",
    "            doclines = [line.rstrip().lower().split(' ') for line in infile]\n",
    "        n_docs = len(doclines)\n",
    "        self.vocab = list({v for doc in doclines for v in doc})\n",
    "        \n",
    "        if stopwords_file:   # if there are stopwords, remove them from vocab\n",
    "            with open(stopwords_file, 'r') as stopfile:\n",
    "                stops = stopfile.read().split()\n",
    "            self.vocab = [x for x in self.vocab if x not in stops]\n",
    "            self.vocab.sort()\n",
    "        \n",
    "        self.documents = []  # create documents\n",
    "        for i in range(n_docs):\n",
    "            self.documents.append({})\n",
    "            for j in range(len(doclines[i])):\n",
    "                if doclines[i][j] in self.vocab:\n",
    "                    self.documents[i][j] = self.vocab.index(doclines[i][j])\n",
    "\n",
    "                    \n",
    "    def initialize(self):\n",
    "        \"\"\" Initializes attributes n_words, n_docs, the three count matrices, \n",
    "            and topics.\n",
    "                        \n",
    "            Note that\n",
    "            n_topics = K, the number of possible topics\n",
    "            n_docs   = M, the number of documents being analyzed\n",
    "            n_words  = V, the number of words in the vocabulary\n",
    "            \n",
    "            To do this, you will need to initialize nkm, nkv, and nk \n",
    "            to be zero arrays of the correct size.\n",
    "            Matrix nkm corresponds to n_(k,m,.)\n",
    "            Matrix nkv corresponds to n_(k,.,v)\n",
    "            Matrix nk corresponds to n_(k,.,.)\n",
    "            You will then iterate through each word found in each document.\n",
    "            In the second of these for-loops (for each word), you will \n",
    "            randomly assign z as an integer from the range of topics.\n",
    "            Then, you will increment each of the count matrices by 1, \n",
    "            given the values for z, m, and w, where w is the index in \n",
    "            vocab of the nth word in document m.\n",
    "            Finally, assign topics as given.\n",
    "        \"\"\"\n",
    "        self.n_words = len(self.vocab)\n",
    "        self.n_docs = len(self.documents)\n",
    "        \n",
    "        # Initialize the three count matrices.\n",
    "        # The (k, m) entry of self.nkm is the number of words in document m assigned to topic k.\n",
    "        self.nkm = np.zeros((self.n_topics, self.n_docs))\n",
    "        # The (k, v) entry of self.nkv is the number of times term v is assigned to topic k.\n",
    "        self.nkv = np.zeros((self.n_topics, self.n_words))\n",
    "        # The (k)-th entry of self.nk is the number of times topic k is assigned in the corpus.\n",
    "        self.nk = np.zeros(self.n_topics)\n",
    "        \n",
    "        # Initialize the topic assignment dictionary.\n",
    "        self.topics = {} # key-value pairs of form (m,i):z\n",
    "        \n",
    "        random_distribution = np.ones(self.n_topics) / self.n_topics\n",
    "        for m in range(self.n_docs):\n",
    "            for i in self.documents[m]:\n",
    "                # Get random topic assignment, i.e. z = ...\n",
    "                # Increment count matrices\n",
    "                # Store topic assignment, i.e. self.topics[(m,i)]=z\n",
    "                raise NotImplementedError(\"Problem 3 Incomplete\")\n",
    "                \n",
    "                \n",
    "    def _sweep(self):\n",
    "        \"\"\" Iterates through each word of each document, giving a better\n",
    "            topic assignment for each word.\n",
    "            \n",
    "            To do this, iterate through each word of each document. \n",
    "            The first part of this method will undo what initialize() did\n",
    "            by decrementing each of the count matrices by 1.\n",
    "            Then, call the method _conditional() to use the conditional \n",
    "            distribution (instead of the uniform distribution used \n",
    "            previously) to pick a more accurate topic assignment z.\n",
    "            Finally, repeat what initialize() did by incrementing each of\n",
    "            the count matrices by 1, but this time using the more \n",
    "            accurate topic assignment.\n",
    "        \"\"\"\n",
    "        for m in range(self.n_docs):\n",
    "            for i in self.documents[m]:\n",
    "                # Retrieve vocab index for i-th word in document m.\n",
    "                # Retrieve topic assignment for i-th word in document m.\n",
    "                # Decrement count matrices.\n",
    "                # Get conditional distribution.\n",
    "                # Sample new topic assignment.\n",
    "                # Increment count matrices.\n",
    "                # Store new topic assignment.\n",
    "                raise NotImplementedError(\"Problem 4 Incomplete\")\n",
    "\n",
    "                \n",
    "    def sample(self, filename, burnin=100, sample_rate=10, n_samples=10, stopwords_file=None):\n",
    "        \"\"\" Runs the Gibbs sampler on the given filename. \n",
    "        \n",
    "            The argument filename is the name and location of a .txt \n",
    "            file, where each line is considered a document.\n",
    "            The corpus is built by method buildCorpus(), and \n",
    "            stopwords are removed (if argument stopwords is provided).\n",
    "            \n",
    "            Initialize attributes total_nkm, total_nkv, and logprobs as\n",
    "            zero arrays.\n",
    "            total_nkm and total_nkv will be the sums of every \n",
    "            sample_rate-th nkm and nkv matrix respectively.\n",
    "            logprobs is of length burnin + sample_rate * n_samples\n",
    "            and will store each log-likelihood after each sweep of \n",
    "            the sampler.\n",
    "            \n",
    "            Burn-in the Gibbs sampler.\n",
    "            \n",
    "            After the burn-in, iterate further for n_samples iterations, \n",
    "            adding nkm and nkv to total_nkm and total_nkv respectively, \n",
    "            but only for every sample_rate-th iteration.\n",
    "            \n",
    "            Also, compute and save the log-likelihood at each iteration \n",
    "            in logprobs using the method _loglikelihood().\n",
    "        \"\"\"\n",
    "        self.buildCorpus(filename, stopwords_file)\n",
    "        self.initialize()\n",
    "        \n",
    "        self.total_nzw = np.zeros((self.n_topics, self.n_words))\n",
    "        self.total_nmz = np.zeros((self.n_docs, self.n_topics))\n",
    "        self.logprobs = np.zeros(burnin + sample_rate * n_samples)\n",
    "        \n",
    "        for i in range(burnin):\n",
    "            # Sweep and store log likelihood.\n",
    "            raise NotImplementedError(\"Problem 5 Incomplete\")\n",
    "        for i in range(sample_rate * n_samples):\n",
    "            # Sweep and store log likelihood\n",
    "            raise NotImplementedError(\"Problem 5 Incomplete\")\n",
    "            if not i % sample_rate:\n",
    "                # accumulate counts\n",
    "                raise NotImplementedError(\"Problem 5 Incomplete\")\n",
    "\n",
    "                \n",
    "    def _conditional(self, m, w):\n",
    "        \"\"\" Returns the conditional distribution given m and w.\n",
    "            Called by _sweep(). \"\"\"\n",
    "        dist = (self.nkm[:,m] + self.alpha) * (self.nkv[:,w] + self.beta) / (self.nk + self.beta * self.n_words)\n",
    "        return dist / np.sum(dist)\n",
    "\n",
    "    def _loglikelihood(self):\n",
    "        \"\"\" Computes and returns the log-likelihood. Called by sample(). \"\"\"\n",
    "        lik = 0\n",
    "\n",
    "        for z in range(self.n_topics):\n",
    "            lik += np.sum(gammaln(self.nkv[z,:] + self.beta)) - gammaln(np.sum(self.nkv[z,:] + self.beta))\n",
    "            lik -= self.n_words * gammaln(self.beta) - gammaln(self.n_words * self.beta)\n",
    "\n",
    "        for m in range(self.n_docs):\n",
    "            lik += np.sum(gammaln(self.nkm[:,m] + self.alpha)) - gammaln(np.sum(self.nkm[:,m] + self.alpha))\n",
    "            lik -= self.n_topics * gammaln(self.alpha) - gammaln(self.n_topics * self.alpha)\n",
    "\n",
    "        return lik\n",
    "    \n",
    "    def phi(self):\n",
    "        \"\"\" Initializes attribute _phi. Called by topterms(). \"\"\"\n",
    "        phi = self.total_nkv + self.beta\n",
    "        self._phi = phi / np.sum(phi, axis=1)[:,np.newaxis]\n",
    "\n",
    "    def theta(self):\n",
    "        \"\"\" Initializes attribute _theta. Called by topterms. \"\"\"\n",
    "        theta = self.total_nkm + self.alpha\n",
    "        self._theta = theta / np.sum(theta, axis=1)[:,np.newaxis]\n",
    "\n",
    "    def topterms(self, n_terms=10):\n",
    "        \"\"\" Returns the top n_terms of each topic found. \"\"\"\n",
    "        self.phi()\n",
    "        self.theta()\n",
    "        vec = np.atleast_2d(np.arange(0, self.n_words))\n",
    "        topics = []\n",
    "        for k in range(self.n_topics):\n",
    "            probs = np.atleast_2d(self._phi[k,:])\n",
    "            mat = np.append(probs, vec, 0)\n",
    "            sind = np.array([mat[:,i] for i in np.argsort(mat[0])]).T\n",
    "            topics.append([self.vocab[int(sind[1, self.n_words - 1 - i])] for i in range(n_terms)])\n",
    "        return topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6\n",
    "\n",
    "Create an `LDACGS` object with $20$ topics, letting $\\alpha$ and $\\beta$ be the default values.\n",
    "Run the Gibbs sampler, with a burn in of $100$ iterations, accumulating $10$ samples, only keeping the results of every $10$th sweep.\n",
    "Use `stopwords.txt` as the stopwords file.\n",
    "\n",
    "Plot the log-likelihoods. How many iterations did it take to burn-in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7\n",
    "\n",
    "Using the method `LDACGS.topterms()`, examine the topics for Reagan's addresses. \n",
    "If `n_topics=20` and `n_samples=10`, you should get the top $10$ words that represent each of the $20$ topics.\n",
    "For each topic, decide what these ten words jointly represent, and come up with a label for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
